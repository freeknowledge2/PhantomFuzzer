#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Web vulnerability analyzer for PhantomFuzzer.

This module provides a specialized analyzer for detecting vulnerabilities in web applications.
"""

import re
from urllib.parse import urlparse, urljoin
from typing import Dict, List, Any, Optional, Union, Set

# Import from phantomfuzzer package
from phantomfuzzer.utils.logging import get_module_logger
from phantomfuzzer.utils.helper import print_info, print_warning, print_error

# Import from vulnerability module
from phantomfuzzer.vulnerability.base_analyzer import BaseAnalyzer
from phantomfuzzer.vulnerability.models import Vulnerability

# Import vulnerability constants
from phantomfuzzer.vulnerability import (
    # Severity levels
    SEVERITY_CRITICAL, SEVERITY_HIGH, SEVERITY_MEDIUM, SEVERITY_LOW, SEVERITY_INFO,
    
    # Vulnerability categories
    CATEGORY_BROKEN_ACCESS_CONTROL, CATEGORY_CRYPTO_FAILURES, CATEGORY_INJECTION,
    CATEGORY_INSECURE_DESIGN, CATEGORY_SECURITY_MISCONFIG, CATEGORY_VULNERABLE_COMPONENTS,
    CATEGORY_AUTH_FAILURES, CATEGORY_SOFTWARE_DATA_INTEGRITY, CATEGORY_LOGGING_MONITORING,
    CATEGORY_SSRF, CATEGORY_XSS, CATEGORY_CSRF, CATEGORY_SQLI, CATEGORY_XXE
)


class WebAnalyzer(BaseAnalyzer):
    """Specialized analyzer for detecting vulnerabilities in web applications."""
    
    def _load_patterns(self) -> Dict[str, Any]:
        """Load vulnerability patterns for web application analysis.
        
        Returns:
            A dictionary of vulnerability patterns.
        """
        # XSS vulnerability patterns
        xss_patterns = {
            'reflected_xss': {
                'pattern': r'(?i)(?:alert|confirm|prompt|eval|document\.write|innerHTML|outerHTML)\s*\(',
                'description': 'Potential reflected XSS vulnerability',
                'severity': SEVERITY_HIGH,
                'category': CATEGORY_XSS,
                'remediation': 'Implement proper output encoding and content security policy.'
            },
            'dom_xss': {
                'pattern': r'(?i)(?:document\.URL|document\.documentURI|location\.href|location\.search|location\.hash|document\.referrer|window\.name)\s*',
                'description': 'Potential DOM-based XSS vulnerability',
                'severity': SEVERITY_HIGH,
                'category': CATEGORY_XSS,
                'remediation': 'Sanitize user input before using it in DOM manipulation.'
            }
        }
        
        # CSRF vulnerability patterns
        csrf_patterns = {
            'missing_csrf_token': {
                'pattern': r'(?i)<form[^>]*>(?:(?!csrf).)*?</form>',
                'description': 'Form without CSRF protection',
                'severity': SEVERITY_MEDIUM,
                'category': CATEGORY_CSRF,
                'remediation': 'Implement CSRF tokens in all forms that modify state.'
            }
        }
        
        # SQL injection patterns
        sqli_patterns = {
            'error_based_sqli': {
                'pattern': r'(?i)(?:SQL syntax.*?MySQL|Warning.*?\\Wmysqli?_|MySQLSyntaxErrorException|valid MySQL result|check the manual that corresponds to your MySQL server version|Unknown column \'[^\']+\' in \'field list\'|MySqlClient\\.|com\\.mysql\\.jdbc|Zend_Db_Statement_Mysqli_Exception)',
                'description': 'Potential SQL injection vulnerability (error-based)',
                'severity': SEVERITY_HIGH,
                'category': CATEGORY_SQLI,
                'remediation': 'Use parameterized queries or prepared statements.'
            }
        }
        
        # Security misconfiguration patterns
        security_misconfig_patterns = {
            'server_info_disclosure': {
                'pattern': r'(?i)(?:Apache/\\d|nginx/\\d|Microsoft-IIS/\\d|PHP/\\d|Server: .*)',
                'description': 'Server information disclosure',
                'severity': SEVERITY_LOW,
                'category': CATEGORY_SECURITY_MISCONFIG,
                'remediation': 'Configure web server to hide version information.'
            },
            'directory_listing': {
                'pattern': r'(?i)<title>Index of /',
                'description': 'Directory listing enabled',
                'severity': SEVERITY_MEDIUM,
                'category': CATEGORY_SECURITY_MISCONFIG,
                'remediation': 'Disable directory listing in web server configuration.'
            }
        }
        
        # Security headers patterns
        security_headers_patterns = {
            'missing_hsts': {
                'header': 'Strict-Transport-Security',
                'description': 'Missing HTTP Strict Transport Security header',
                'severity': SEVERITY_MEDIUM,
                'category': CATEGORY_SECURITY_MISCONFIG,
                'remediation': 'Add Strict-Transport-Security header with appropriate max-age.'
            },
            'missing_xframe': {
                'header': 'X-Frame-Options',
                'description': 'Missing X-Frame-Options header',
                'severity': SEVERITY_MEDIUM,
                'category': CATEGORY_SECURITY_MISCONFIG,
                'remediation': 'Add X-Frame-Options header with DENY or SAMEORIGIN value.'
            },
            'missing_xss_protection': {
                'header': 'X-XSS-Protection',
                'description': 'Missing X-XSS-Protection header',
                'severity': SEVERITY_LOW,
                'category': CATEGORY_SECURITY_MISCONFIG,
                'remediation': 'Add X-XSS-Protection header with 1; mode=block value.'
            },
            'missing_content_type_options': {
                'header': 'X-Content-Type-Options',
                'description': 'Missing X-Content-Type-Options header',
                'severity': SEVERITY_LOW,
                'category': CATEGORY_SECURITY_MISCONFIG,
                'remediation': 'Add X-Content-Type-Options header with nosniff value.'
            },
            'missing_csp': {
                'header': 'Content-Security-Policy',
                'description': 'Missing Content Security Policy header',
                'severity': SEVERITY_MEDIUM,
                'category': CATEGORY_SECURITY_MISCONFIG,
                'remediation': 'Implement a Content Security Policy to mitigate XSS and data injection attacks.'
            }
        }
        
        return {
            'xss': xss_patterns,
            'csrf': csrf_patterns,
            'sqli': sqli_patterns,
            'security_misconfig': security_misconfig_patterns,
            'security_headers': security_headers_patterns
        }
    
    def analyze(self, target: str, scan_context: Optional[Dict[str, Any]] = None) -> List[Vulnerability]:
        """Analyze a web application for vulnerabilities.
        
        Args:
            target: The URL of the web application to analyze.
            scan_context: Optional context information about the scan.
        
        Returns:
            A list of detected vulnerabilities.
        """
        # Validate URL
        if not self._is_valid_url(target):
            self.logger.error(f"Invalid URL: {target}")
            print_error(f"Invalid URL: {target}")
            return []
        
        # Initialize list to store detected vulnerabilities
        vulnerabilities = []
        
        try:
            # Import requests library (only when needed)
            try:
                import requests
                from requests.exceptions import RequestException
            except ImportError:
                self.logger.error("Requests library not available. Cannot analyze web application.")
                print_error("Requests library not available. Cannot analyze web application.")
                return []
            
            # Set up session with proper headers
            session = requests.Session()
            session.headers.update({
                'User-Agent': 'PhantomFuzzer/1.0 Web Vulnerability Scanner',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'max-age=0'
            })
            
            # Make initial request to the target
            self.logger.info(f"Analyzing web application: {target}")
            print_info(f"Analyzing web application: {target}")
            
            response = session.get(target, verify=False, timeout=10)
            
            # Check response status
            if response.status_code >= 400:
                self.logger.warning(f"Received HTTP {response.status_code} from {target}")
                print_warning(f"Received HTTP {response.status_code} from {target}")
            
            # Check security headers
            self._check_security_headers(target, response.headers, vulnerabilities)
            
            # Check for content-based vulnerabilities
            self._check_content_vulnerabilities(target, response.text, vulnerabilities)
            
            # Crawl the site if requested
            if scan_context and scan_context.get('crawl', False):
                self._crawl_and_analyze(target, session, vulnerabilities, scan_context)
            
            # Use ML for enhanced detection if enabled
            if self.ml_enabled and self.ml_integration:
                self._analyze_with_ml(target, response.text, vulnerabilities)
            
            return vulnerabilities
            
        except Exception as e:
            self._handle_exception(e, f"Error analyzing web application {target}")
            return vulnerabilities
    
    def _is_valid_url(self, url: str) -> bool:
        """Check if a URL is valid.
        
        Args:
            url: The URL to check.
        
        Returns:
            True if the URL is valid, False otherwise.
        """
        try:
            result = urlparse(url)
            return all([result.scheme, result.netloc]) and result.scheme in ['http', 'https']
        except Exception:
            return False
    
    def _check_security_headers(self, url: str, headers: Dict[str, str], vulnerabilities: List[Vulnerability]) -> None:
        """Check for missing or misconfigured security headers.
        
        Args:
            url: The URL being analyzed.
            headers: The HTTP response headers.
            vulnerabilities: The list to add detected vulnerabilities to.
        """
        try:
            # Convert header names to lowercase for case-insensitive comparison
            headers_lower = {k.lower(): v for k, v in headers.items()}
            
            for name, header_info in self.patterns['security_headers'].items():
                header_name = header_info['header'].lower()
                
                if header_name not in headers_lower:
                    # Create a vulnerability for missing header
                    vulnerabilities.append(Vulnerability(
                        name=f"Missing {header_info['header']} Header",
                        description=header_info['description'],
                        severity=header_info['severity'],
                        location=url,
                        category=header_info['category'],
                        evidence=f"Header not found in response",
                        remediation=header_info['remediation'],
                        detection_method="header_check"
                    ))
                # Could add checks for misconfigured headers here
        except Exception as e:
            self._handle_exception(e, f"Error checking security headers for {url}")
    
    def _check_content_vulnerabilities(self, url: str, content: str, vulnerabilities: List[Vulnerability]) -> None:
        """Check for vulnerabilities in page content.
        
        Args:
            url: The URL being analyzed.
            content: The HTML content of the page.
            vulnerabilities: The list to add detected vulnerabilities to.
        """
        try:
            # Check for XSS vulnerabilities
            for name, pattern_info in self.patterns['xss'].items():
                if re.search(pattern_info['pattern'], content):
                    vulnerabilities.append(Vulnerability(
                        name=f"Potential XSS Vulnerability",
                        description=pattern_info['description'],
                        severity=pattern_info['severity'],
                        location=url,
                        category=pattern_info['category'],
                        evidence=f"Matched pattern: {pattern_info['pattern']}",
                        remediation=pattern_info['remediation'],
                        detection_method="pattern_matching"
                    ))
            
            # Check for CSRF vulnerabilities
            for name, pattern_info in self.patterns['csrf'].items():
                if re.search(pattern_info['pattern'], content):
                    vulnerabilities.append(Vulnerability(
                        name=f"Potential CSRF Vulnerability",
                        description=pattern_info['description'],
                        severity=pattern_info['severity'],
                        location=url,
                        category=pattern_info['category'],
                        evidence=f"Form without CSRF token detected",
                        remediation=pattern_info['remediation'],
                        detection_method="pattern_matching"
                    ))
            
            # Check for SQL injection vulnerabilities
            for name, pattern_info in self.patterns['sqli'].items():
                if re.search(pattern_info['pattern'], content):
                    vulnerabilities.append(Vulnerability(
                        name=f"Potential SQL Injection Vulnerability",
                        description=pattern_info['description'],
                        severity=pattern_info['severity'],
                        location=url,
                        category=pattern_info['category'],
                        evidence=f"SQL error message detected",
                        remediation=pattern_info['remediation'],
                        detection_method="pattern_matching"
                    ))
            
            # Check for security misconfigurations
            for name, pattern_info in self.patterns['security_misconfig'].items():
                if re.search(pattern_info['pattern'], content):
                    vulnerabilities.append(Vulnerability(
                        name=f"Security Misconfiguration",
                        description=pattern_info['description'],
                        severity=pattern_info['severity'],
                        location=url,
                        category=pattern_info['category'],
                        evidence=f"Matched pattern: {pattern_info['pattern']}",
                        remediation=pattern_info['remediation'],
                        detection_method="pattern_matching"
                    ))
        except Exception as e:
            self._handle_exception(e, f"Error checking content vulnerabilities for {url}")
    
    def _crawl_and_analyze(self, base_url: str, session, vulnerabilities: List[Vulnerability], scan_context: Dict[str, Any]) -> None:
        """Crawl a website and analyze each page.
        
        Args:
            base_url: The base URL to crawl from.
            session: The requests session to use.
            vulnerabilities: The list to add detected vulnerabilities to.
            scan_context: Context information about the scan.
        """
        try:
            # Import BeautifulSoup (only when needed)
            try:
                from bs4 import BeautifulSoup
            except ImportError:
                self.logger.error("BeautifulSoup library not available. Cannot crawl website.")
                print_error("BeautifulSoup library not available. Cannot crawl website.")
                return
            
            # Set up crawling parameters
            max_depth = scan_context.get('max_depth', 2)
            max_pages = scan_context.get('max_pages', 20)
            
            # Keep track of visited URLs and URLs to visit
            visited_urls = set([base_url])
            urls_to_visit = [(base_url, 0)]  # (url, depth)
            
            # Crawl the website
            while urls_to_visit and len(visited_urls) < max_pages:
                current_url, depth = urls_to_visit.pop(0)
                
                # Skip if we've reached max depth
                if depth >= max_depth:
                    continue
                
                try:
                    # Make request to the current URL
                    response = session.get(current_url, verify=False, timeout=10)
                    
                    # Skip non-HTML responses
                    if 'text/html' not in response.headers.get('Content-Type', ''):
                        continue
                    
                    # Parse HTML
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # Check for vulnerabilities in the current page
                    self._check_security_headers(current_url, response.headers, vulnerabilities)
                    self._check_content_vulnerabilities(current_url, response.text, vulnerabilities)
                    
                    # Extract links
                    if depth < max_depth - 1:
                        for link in soup.find_all('a', href=True):
                            href = link['href']
                            
                            # Skip empty links, anchors, and non-HTTP links
                            if not href or href.startswith('#') or href.startswith('javascript:') or href.startswith('mailto:'):
                                continue
                            
                            # Resolve relative URLs
                            absolute_url = urljoin(current_url, href)
                            
                            # Skip external links and already visited URLs
                            if not absolute_url.startswith(base_url) or absolute_url in visited_urls:
                                continue
                            
                            # Add to list of URLs to visit
                            visited_urls.add(absolute_url)
                            urls_to_visit.append((absolute_url, depth + 1))
                
                except Exception as e:
                    self.logger.warning(f"Error crawling {current_url}: {str(e)}")
                    continue
        
        except Exception as e:
            self._handle_exception(e, f"Error during crawling of {base_url}")
    
    def _analyze_with_ml(self, url: str, content: str, vulnerabilities: List[Vulnerability]) -> None:
        """Use ML to enhance vulnerability detection.
        
        Args:
            url: The URL being analyzed.
            content: The HTML content of the page.
            vulnerabilities: The list to add detected vulnerabilities to.
        """
        try:
            if self.ml_integration:
                # Get ML predictions
                ml_vulnerabilities = self.ml_integration.analyze_web(content, url)
                
                # Add ML-detected vulnerabilities to the list
                for vuln in ml_vulnerabilities:
                    vulnerabilities.append(Vulnerability(
                        name=vuln['name'],
                        description=vuln['description'],
                        severity=vuln['severity'],
                        location=url,
                        category=vuln.get('category'),
                        evidence=vuln.get('evidence'),
                        remediation=vuln.get('remediation'),
                        detection_method="machine_learning",
                        confidence=vuln.get('confidence', 0.8)
                    ))
        except Exception as e:
            self._handle_exception(e, f"Error during ML analysis of {url}")
